{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Her skal det være en relevant overskrift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innhenter verktøy fra bibliotek\n",
    "Import-stegene henter inn bibliotek med kode og funksjoner utviklet ekstern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kjører metoden read.path for å få oversikt over hvilke parquet datasett som er tilrettelagt i tilknytning veilderens lagringsområde i sky.  Oversikt blir lest inn i egen dataframe - df_datsets.\n",
    "Aktuelt lagringsområde blir lagt inn som parameter (string objekt som vi definerer selv) INNDATA_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INNDATA_PATH = '/felles/veiledning/pyspark/eksempler/*'\n",
    "df_datasets = spark.read.path(INNDATA_PATH)\n",
    "#df_datasets = spark.read.path(INNDATA_PATH+'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_datsets skrives ut i output vindu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avleder variabel filname fra variabel path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets = df_datasets.withColumn('filename',F.substring(df_datasets.path, len(INNDATA_PATH), 30))\n",
    "df_datasets.show(100, False)\n",
    "\n",
    "\n",
    "#df_datasets = df_datasets.withColumn('filename',F.length(df_datasets.path))\n",
    "#df_datasets = df_datasets.withColumn('filename',df_datasets.path[len(INNDATA_PATH):20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selketerer ut rader bestående av de filnavn som vi ønsker å trekke ut for videre bearbeiding. \n",
    "Viser hvordan man kan bruke liste (Phyton list) for å gjøre en slik seleksjon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['areal', 'bnp', 'innbyggerantall/2020']\n",
    "df_datasets = df_datasets.filter(df_datasets.filename.isin(filenames))\n",
    "df_datasets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppretter dictionaryet dict_df og leser så de tre parquet datsettene i output vindu over inn som dataframes i dictionaryet. \n",
    "Filnavnene blir keys i dictionaryet. Avslutningsvis i pargraf skrives key-navnene ut i output ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = {}\n",
    "for row in df_datasets.rdd.collect():\n",
    "    dict_df[row.filename] = spark.read.path(row.path)\n",
    "for key in dict_df:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skriver ut innhold i dataframene i dictionaryet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Areal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df['areal'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df['areal'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tar ut oversikt over de radene i kolonnen \"Land\" som er duplikater\n",
    "dict_df['areal'].exceptAll(dict_df['areal'].dropDuplicates(['Land'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df['bnp'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innbyggerantall 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df['innbyggerantall/2020'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fjerner duplikater og kolonner vi ikke ønsker å ha med videre\n",
    "Vi ønsker å sette sammen informasjon fra datasettene i dictinaryet til et nytt datasett. Dette vil vi gjøre ved å koble de sammen via Landkode variabel. Vi fjerner eventuelle dublikater på Landkode og dropper variable som vi ikke trenger/ønsker å ha med på det nye datasettet i innledende steg under.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Areal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df['areal'] = dict_df['areal'].dropDuplicates(['Landkode']).drop('kilde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BNP (Legger inn variable som skal droppes fra dataframe i phyton tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_drop_kolonner = ('Land', 'Kilde', 'År')\n",
    "dict_df['bnp'] = dict_df['bnp'].drop_duplicates(['Landkode']).drop(*liste_drop_kolonner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innbyggerantall (Legger inn variable som skal droppes fra datafrma i phyton tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_drop_kolonner = ['Land', 'Kilde', 'År']\n",
    "dict_df['innbyggerantall/2020'] = dict_df['innbyggerantall/2020'].drop_duplicates(['Landkode']).drop(*liste_drop_kolonner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kobler sammen datsettene og etablerer en dataframe vi skal utføre statistisk analyse på. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kobler først areal og bnp og oppretter dataframen df_areal_bnp. Vi legger ikke denne dataframen inn i dictionaryet. Det hadde imidlertid enkelt latt seg gjøre ved å skrive f.eks. å bytte ut df_areal_bnp med dict['areal_bnp]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areal_bnp = dict_df['areal'].join(dict_df['bnp'], 'Landkode', how='inner')\n",
    "df_areal_bnp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kobler derettet df_areal_bnp med innbyggerantall/2020 dataframen (i dictionaryet). Dermed får vi produsert den dataframen som vi ænsker å utføre en nærmere statistk analyse på. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sammensatt = df_areal_bnp.join(dict_df['innbyggerantall/2020'], 'Landkode', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skriver ut innhold i df_sammensatt i output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sammensatt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrer dataframe som parquet datasett på Dapla\n",
    "Skriver pyspark dataframe til GCS bucket i parquet format\n",
    "Skriver til hjemmekatalog under /user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sammensatt.write\\\n",
    "    .option(\"valuation\", \"INTERNAL\")\\\n",
    "    .option(\"state\", \"INPUT\")\\\n",
    "    .path('/user/matz.ivan.faldmo/df_sammensatt_pyspark')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.path('/user/matz.ivan.faldmo/*')\n",
    "ds.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

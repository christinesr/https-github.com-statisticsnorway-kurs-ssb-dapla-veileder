{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beregning av statistiske størrelser med pyspark\n",
    "### I denne noten viser vi hvordan vi kan beregne statistiske størrelser i pyspark. Utgangspunkt for beregninger er parquet datasett som leses inn fra dapla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innhenter verktøy fra bibliotek\n",
    "Import-stegene henter inn bibliotek med kode og funksjoner utviklet ekstern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kjører metoden read.path for å få oversikt over hvilke parquet datasett som er tilrettelagt i tilknytning veilderens lagringsområde i sky.  Oversikt blir lest inn i egen dataframe - df_datsets.\n",
    "Aktuelt lagringsområde blir lagt inn som parameter (string objekt som vi definerer selv) INNDATA_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INNDATA_PATH = '/felles/veiledning/datasett/*'\n",
    "df_datasets = spark.read.path(INNDATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_datsets skrives ut i output vindu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet datasettene df_sammensatt_pyspark og df_sammensatt_python over skal i utgangspunktet være identiske. De er produsert i henholdsvis pyspark og python. I vårt eksempel tar vi utgangspunkt i df_sammensatt_pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leser inn parquet datasett\n",
    "Leser inn parquet datsett df_sammensatt_pyspark, selekterer de variable vi skal bruke og etablerer dataframen df_areal_bnp_og_innbyggerantall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areal_bnp_og_innbyggerantall = spark.read.path('/felles/veiledning/datasett/df_sammensatt_pyspark').select('Land', 'Areal', 'BNP', 'Innbyggerantall')\n",
    "df_areal_bnp_og_innbyggerantall.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beregninger - eksempel 1\n",
    "Vi ønsker som utgangspunkt å beregne størrelsene count, mean, std, min, max og median for alle variable i df_areal_bnp_og_innbyggerantall. Ved å kjøre metoden summary på datasettet får vi ut  størrelsene count, mean, std, min, max, 25% percentile, 50% percentile og 75% percentile.  Vi betrakter 50% percentile som medianen. Dvs at vi får ut alle de størrelser vi ønsker å beregne i dette eksenpelet kun ved å describe metoden i pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Kjører ut statistiske størrelser\n",
    "Kjører ut statistike størrelser fra datframe df_areal_bnp_og_innbyggerantall med metoden summary. Resultat fra metode blir lagt i dataframeen df_stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df_areal_bnp_og_innbyggerantall.summary()\n",
    "df_stat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Transformerer df_stat \n",
    "Transformerer df_stat (med bl.a. pivot funksjonen) slik at rader i df_stat blir kolonner i dataframe df_stat_piv. Formål er å gjøre output mer oversiktlig ved at alle størrelser på en gitt variabel blir liggende i samme rad. Dette kan være hensiktsmessig i de tilfeller antall \"statistikkvaiable\" er høyt (dvs langt høyere enn de fire som inngår i eksempel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lst = list(df_stat.columns)\n",
    "var_lst.remove('summary')\n",
    "i = 1\n",
    "for col in var_lst:\n",
    "    df_tmp = df_stat.groupBy().pivot('summary').agg(F.sum(col)).withColumn('variabel',F.lit(col))\n",
    "    if i == 1:\n",
    "        df_stat_piv = df_tmp \n",
    "    else:\n",
    "        df_stat_piv = df_stat_piv.unionByName(df_tmp)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_piv.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beregninger eksempel 2\n",
    "I eksempel 1 fikk vi beregnet de størrelsene vi var ute etter kun ved å bruke metoden summary. Hvis vi ønsker å beregne størrelser som ikke er støttet av summary metoden kan vi bruke andre bibliotek - f.eks. statistics bilioteket.\n",
    "\n",
    "I eksempel 2 ønsker vi å beregne medianen gjennom å bruke en fuksjon/metode fra statistics bibloteket. Videre ønsker vi i samme steg å sette sammen medianen \"sammen\" med størrelsene vi får ved å kjøre summary metoden. Resultat skal transformeres på tilsvarende måte som i eksempel 1.\n",
    "\n",
    "Vi ønsker å etablere datasett hvor medianen er satt sammen med størrelsene vi får fra summary. For å få til dette må vi gjennomføre flere steg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Kjører ut statistiske størrelser med describe\n",
    "Kjører ut statistike størrelser fra datframe df_areal_bnp_og_innbyggerantall med metoden describe. Resultat fr metode blir lagt i dataframeen df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_adv = df_areal_bnp_og_innbyggerantall.summary()\n",
    "df_stat_adv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printer schemaet til df_stat_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_adv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Beregner medianen\n",
    "Beregner medianen for hver kolonne i df_areal_bnp_og_innbyggerantall med funksjonen approxQuantile.\n",
    "Resultat fra funsjon blir returnert inn i values felt i dictinaryet dict_median der key-value blir initiert med navn på kolonne i var_lst (kolonnenavnene i stat_df og de vi også skal beregne median for). Beregninger gjøres i loop på en og en variabel. Man må sjekke datatypen på kolonnen. approxQuantile feiler hvis datatype ikke er av numerisk type. Etter gjenomløp av loop blir dictinaryet transformert til et row-objekt som deretter blir transformert til et rdd-objekt før vi til slutt får etablert dataframen df_median ved å utføre metoden toDF på row-objektet. For å få like datatyper på df_stat og df_median (noe vi må ha for å få slått de sammen) endrer vi datatypen til de variabler i df_median som har ArrayType til String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lst = list(df_stat_adv.columns)\n",
    "var_lst.remove('summary')\n",
    "dict_median = {}\n",
    "for col in var_lst:\n",
    "    if (isinstance(df_areal_bnp_og_innbyggerantall.schema[col].dataType, LongType)) | (isinstance(df_areal_bnp_og_innbyggerantall.schema[col].dataType, IntegerType)):\n",
    "        dict_median[col] = df_areal_bnp_og_innbyggerantall.approxQuantile(col, [0.5], 0) \n",
    "    else:\n",
    "        dict_median[col] = \"null\" \n",
    "dict_median['summary'] = 'median'        \n",
    "median_row = Row(dict_median)   \n",
    "rdd = sc.parallelize(median_row)\n",
    "df_median = rdd.toDF()\n",
    "for col in var_lst:\n",
    "    if (isinstance(df_median.schema[col].dataType, ArrayType)):\n",
    "        df_median = df_median.withColumn(col, df_median[col].getItem(0).cast(\"string\"))  \n",
    "df_median.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Slår sammen df_stat og df_median\n",
    "Slår sammen df_stat og df_median. Viktig at man her bruker unionByName fordi rekkefølgen på variablene ikke er identisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_sammensatt = df_stat_adv.unionByName(df_median)\n",
    "df_stat_sammensatt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Transformerer df_stat_sammensatt \n",
    "Transformerer df_stat (med bl.a. pivot funksjonen) slik at rader i df_stat blir kolonner i dataframe df_stat_piv. Formål er å gjøre output mer oversiktlig ved at alle størrelser på en gitt variabel blir liggende i samme rad. Dette kan være hensiktsmessig i de tilfeller antall \"statistikkvaiable\" er høyt (dvs langt høyere enn de fire som inngår i eksempel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lst = list(df_stat_sammensatt.columns)\n",
    "var_lst.remove('summary')\n",
    "i = 1\n",
    "for col in var_lst:\n",
    "    df_tmp = df_stat_sammensatt.groupBy().pivot('summary').agg(F.sum(col)).withColumn('variabel',F.lit(col))\n",
    "    if i == 1:\n",
    "        df_stat_sammensatt_piv = df_tmp \n",
    "    else:\n",
    "        df_stat_sammensatt_piv = df_stat_sammensatt_piv.unionByName(df_tmp)\n",
    "    i = i + 1\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_sammensatt_piv.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finne, se og lagre data på Dapla\n",
    "Det vil være ulike måter brukeren kan finne dataene på Dapla. I grove trekk kan vi skille på søk i metadata (beskrivelsene av dataene) og utforske i en filstruktur.\n",
    "\n",
    "### Lese data fra Dapla\n",
    "Beskrivelse av hvordan du kan utforske og lese inn data fra Dapla.\n",
    "* [Lese eksempeldata fra dataplattformen (PySpark)](Lese%20eksempeldata%20fra%20dataplattformen%20(PySpark).ipynb)\n",
    "\n",
    "### Lagre data på Dapla\n",
    "Beskrivelse av hvordan du kan konstruere en dataframe og lagre den på Dapla.\n",
    "* [Opprette og lagre eksempeldata på Dapla (PySpark)](Opprette%20og%20skrive%20PySpark%20DataFrame%20til%20felles%20datalager.ipynb)\n",
    "\n",
    "### Se og operere på dataene (via DataFrame-objekter)\n",
    "I både PySpark, Python og SparkR kan datasett håndteres via et <code>DataFrame</code> <b>objekt</b>.\n",
    "\n",
    "Se og lær:\n",
    "* [Intro til Pyspark.sql.Dataframe](Intro%20til%20Pyspark.sql.Dataframe.ipynb)\n",
    "* [Intro til Pandas DataFrame](Intro%20til%20Pandas%20DataFrame.ipynb)\n",
    "* [Lese inn og skrive dataframes i R](Lese%20inn%20og%20skrive%20dataframes%20i%20R.ipynb)\n",
    "\n",
    "### Få oversikt over innhold i dataframes \n",
    "Tips om funksjoner i Python og PySpark som gir tilsvarende informasjon som PROC CONTENTS i SAS. \n",
    "* [Få oversikt over innhold i dataframes med Python og Pyspark.](F%C3%A5%20oversikt%20over%20innhold%20i%20dataframes%20med%20Python%20og%20PySpark.ipynb)\n",
    "\n",
    "### Daplas Python-pakke\n",
    "Dapla har utviklet en egen Python pakke for å kunne kommunisere direkte med Dapla-tjenester og jobbe med datasett uten å bruke spark. Inneholder blant annet funksjonalitet for å lese datasett fra Google Cloud Storage (GCS) inn som en Pandas dataframe, skrive innholdet i en Pandas dataframe til en \"bøtte\" i GCS, vise innhold i en GCS \"bøtte\" og lete etter datasett.\n",
    "* [Dapla Python pakke.](Dapla%20python%20pakke.ipynb)\n",
    "\n",
    "### Innlesing og behandling av flere datasett i samme notebook\n",
    "Eksempler i Pyspark og Python (med Pandas) som viser hvordan vi man kan lese inn og behandle et ukjent antall inndatasett ved å bruke python collection datatypes (list, dictionary og tuple) og python for loops for å operasjonalisere steg i en tenkt klargjørings eller analyseprosess.\n",
    "* [Innlesing og behandling av datasett - Python med Pandas.](Innlesing%20og%20behandling%20av%20datasett%20med%20Pyton%20og%20Pandas.ipynb)\n",
    "* [Innlesing og behandling av datasett - Pyspark.](Innlesing%20og%20behandling%20av%20datasett%20med%20Pyspark.ipynb)\n",
    "\n",
    "### Deskriptiv statistikk \n",
    "Eksempler i Pyspark og Python (med Pandas) som viser hvordan vi kan beregne statistiske størrelser med innebygde funksjonner og metoder i verktøy og biblioteker.\n",
    "* [Deskriptiv statistikk - Python med Pandas.](Deskriptiv%20statistikk%20med%20Python%20og%20Pandas.ipynb)\n",
    "* [Deskriptiv statistikk - Pyspark.](Deskriptiv%20statistikk%20med%20Pyspark.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

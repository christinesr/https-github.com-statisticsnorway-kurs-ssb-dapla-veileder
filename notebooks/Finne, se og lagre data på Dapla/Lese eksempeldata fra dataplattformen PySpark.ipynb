{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lese eksempeldata fra dataplattformen (PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hente data fra datalager\n",
    "For å hente data fra datalager må du vite hvor dataene ligger. Hvert statistikkteam har sine egne såkalte\n",
    "<i>bøtter</i> i Google som heter Google Cloud Storage (GCS). Bøttene følger en navnekonvensjon på følgende måte:\n",
    "\n",
    "Produksjonsmiljø:\n",
    "<font size=2>\n",
    "- <code>ssb-prod-<strong>teamnavn</strong>-data-kilde</code>: Pseudonymiserte rådata fra datakildene\n",
    "- <code>ssb-prod-<strong>teamnavn</strong>-data-produkt</code>: Data knyttet til statistikkproduktet, med følgende underkataloger:\n",
    "    - <code>inndata</code>\n",
    "    - <code>klargjorte-data</code>\n",
    "    - <code>statistikk</code>\n",
    "    - <code>utdata</code>\n",
    "- <code>ssb-prod-<strong>teamnavn</strong>-data-delt</code>: Data knyttet til statistikkproduktet som kan deles med andre statistikkteam.\n",
    "Disse vil ha følgende underkataloger:\n",
    "    - <code>inndata</code>\n",
    "    - <code>klargjorte-data</code>\n",
    "    - <code>statistikk</code>\n",
    "    - <code>utdata</code>\n",
    "</font>\n",
    "\n",
    "På samme måte som i produksjonsmiljøet finnes det bøtter for utviklings- og testformål:\n",
    "<font size=2>\n",
    "- <code>ssb-staging-<strong>teamnavn</strong>-data-kilde</code>\n",
    "- <code>ssb-staging-<strong>teamnavn</strong>-data-produkt</code>\n",
    "- <code>ssb-staging-<strong>teamnavn</strong>-data-delt</code>\n",
    "</font>\n",
    "\n",
    "I tillegg til disse finnes det noen bøtter med data som kan deles med alle i SSB og og som kan brukes til kurs og\n",
    "opplæring (bl.a. denne veilederen). Disse bøttene er:\n",
    "<font size=2>\n",
    "- <code>ssb-prod-dapla-data-delt</code>\n",
    "- <code>ssb-staging-dapla-data-delt</code>\n",
    "</font>\n",
    "\n",
    "\n",
    "### Dapla Python-bibliotek\n",
    "\n",
    "Dapla har samlet flere hjelpefunksjoner i en egen Python-pakke. Denne importeres slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dapla as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utforske data på dataplattformen\n",
    "\n",
    "Denne veilederen vil bruke data fra mappen\n",
    "<code>gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/pyspark/eksempler</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se etter eksempeldata\n",
    "Kjør følgende kode for å liste ut alle mappende under <code>eksempler</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.show(\"gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/pyspark/eksempler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hente eksempeldata via Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areal = dp.read_pandas(\"gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/pyspark/eksempler/bnp\")\n",
    "df_areal.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hente eksempeldata via Spark\n",
    "I Jupyter kan man velge mellom flere såkalte <i>kernels</i> (kjerner). Velger man en Pyspark kernel vil man automatisk få\n",
    "instansiert et <code>spark</code> objekt som kan brukes for å lese, prosessere og skrive store menger data. Du kan lese\n",
    "mer om hvilke muligheter som finnes i Spark [her](https://spark.apache.org).\n",
    "\n",
    "#### Lese data inn i en Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spark-objektet er allerede instansiert i pyspark-kernel.\n",
    "df_areal = spark.read.parquet(\"gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/pyspark/eksempler/areal\")\n",
    "df_areal.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lese data inn i som en Pandas dataframe\n",
    "\n",
    "Fra og med Spark versjon 3.2 er det støtte for å lese data fra Spark direkte inn som Pandas dataframes ved å bruke\n",
    "[Pandas API on Spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html). Da vil\n",
    "man kunne lese inn data slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "df_areal = ps.read_parquet(\"gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/pyspark/eksempler/areal\")\n",
    "df_areal.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
